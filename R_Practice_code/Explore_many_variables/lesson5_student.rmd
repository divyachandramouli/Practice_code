Lesson 5
========================================================

### Multivariate Data
To analyze two variables - scatterplot works
Third variable - add another way to represent it

***

### Moira Perceived Audience Size Colored by Age
Perceived audience size vs actual audience size
3rd var added: age
Perceived audience size by age plot - add color to represent age
- again getting horizontal stripes - no pattern in color - overplotting 
Can't tell if older people guessed better than younger people
Sometimes it's a dead end

***

### Third Qualitative Variable
Examine the relationship between friend count and age by adding a third variable - age

Without adding age as a variable:
```{r}
library(ggplot2)
```

```{r}

ggplot(aes(friend_count),
       data = subset(pf, !is.na(gender))) +
  geom_histogram()+
  facet_wrap(~gender)
```

Statistics for friend_count by gender
```{r}
by(pf$friend_count,pf$gender,summary)
```
Comments:
It appears that females have more friends on average than male users
Is this because female users have a different age distribution??
Maybe conditional on age, the differences are actually larger?

```{r Third Qualitative Variable}
library(ggplot2)
pf <- read.csv('pseudo_facebook.tsv',sep='\t')
ggplot(aes(x = gender, y = age),
       data = subset(pf, !is.na(gender))) + geom_boxplot()+
  stat_summary(fun.y=mean, geom = 'point', shape =4)
```
Averages are marked by an X, because shape =4 
Since male users are a bit younger, we might think that a simple male female comparison doesn't capture the differences in friend count

Let's look at median friend count by age and gender instead:

```{r}
ggplot(aes(x=age, y = friend_count),data = subset(pf, !is.na(gender))) + geom_line(aes(color=gender),stat='summary',fun.y=median)
```

Almost throughout, the median friend count for women is larger than for men, with exceptions that include noisy estimates for very "old" users- we are not confident about these reported ages
Users reporting to be 70 have the same friend count regardless of gender


# Write code to create a new data frame,called 'pf.fc_by_age_gender', that contains
# information on each age AND gender group.

# The data frame should contain the following variables:

#    mean_friend_count,
#    median_friend_count,
#    n (the number of users in each age and gender grouping)

summarise will remove one layer of grouping when it runs ( the last group - gender)
We need to ungroup one more time to remove the age layer
Arrange the dataframe by age

```{r}
library(dplyr)
#sub_pf=subset(pf, !is.na(pf$gender)) - can use this dataframe instead of filtering

pf.fc_by_age_gender <- pf %>%
  filter(!is.na(gender)) %>%
  group_by(age,gender) %>%
  summarise(mean_friend_count = (mean(friend_count)), median_friend_count = round(median(as.numeric(friend_count))), n=n())%>%
   ungroup() %>%
  arrange(age)

pf.fc_by_age_gender 
```


***

### Plotting Conditional Summaries
Notes:
### Construct the plot for the data from pf.fc_by_age_gender
### Plot median friend count for each gender as age increases
```{r Plotting Conditional Summaries}
ggplot(aes(x=age, y = median_friend_count),data = pf.fc_by_age_gender) + geom_line(aes(color=gender))
```
### Thinking in Ratios
Understand more about about how the difference between male and female friend counts varies with age

Gender difference is largest for the young users
How many times more friends does the average female user have than the male user??

***


Notes:

***

### Wide and Long Format
the grouped data frame is in Long format 
- The variable grouped over - male and female has been repeated for each age
It has 2 rows male and female for each age
Convert it to a wide format 
-Should have one row for each age, put the median friend count for male and female in the next two columns
age | male| female with male and female holding the median friend counts

***

### Reshaping Data
Notes:

```{r}
install.packages("tidyr")
library(tidyr)

spread(subset(pf.fc_by_age_gender, 
       select = c('gender', 'age', 'median_friend_count')), 
       gender, median_friend_count)
```
Using reshape2 package
We want  adataframe so we'll use dcast
To get an array or a matrix use acast 

dcast( <dataframe to be changed> , variables you want to keep with a plus sign - (here you want to keep only age) ~ variables whose categories you want as columns (gender) , value.var = <variable with key measurements of the values in the df>)

```{r}
#install.packages('reshape2')
library(reshape2)

pf.fc_by_age_gender.wide <- dcast(pf.fc_by_age_gender, age~ gender, value.var= 'median_friend_count')
head(pf.fc_by_age_gender.wide)

```
 melt - allows conversion from wide to back to the long format
 
 It's important to use quotes around the variable name that is assigned tovalue.var. 

### We could also create a similar data frame using the dplyr and tidyr packages:

```{r}
pf.fc_by_age_gender.wide.alt <-
  subset(pf.fc_by_age_gender[c('age', 'gender', 'median_friend_count')],
         !is.na(gender)) %>%
  spread(gender, median_friend_count) %>%
  mutate(ratio = male / female)

head(pf.fc_by_age_gender.wide)

```

***

### Ratio Plot

# Plot the ratio of the female to male median friend counts vs age
# using the data frame pf.fc_by_age_gender.wide 

# Think about what geom you should use.Add a horizontal line to the plot with
# a y intercept of 1, which will be the base line. Look up the documentation
# for geom_hline to do that. Use the parameter linetype in geom_hline to make the
# line dashed.

# The linetype parameter can take the values 0-6:
# 0 = blank, 1 = solid, 2 = dashed 3 = dotted, 4 = dotdash, 5 = longdash 6 = twodash
```{r Ratio Plot}
ggplot(aes(x=age, y= female/male), data=pf.fc_by_age_gender.wide)+geom_line()+geom_hline(yintercept =1, alpha=0.3, linetype = 2)
```
In young female users, median female user has about 2.5 times as many friends as the median male user,
Clearly it was helpful to condition with age in understanding the relationship of friend count with gender
The pattern is robust for users of many different ages and highlights where the difference is striking 
there are many processes that can produce this difference - including the biased distribution from which the pseudo facebook data was generated

One idea that shows the complexity of our interpretation is - people from particular countries who joined facebook recently are more likely to be male with lower friend counts




***

### Third Quantitative Variable
We looked at ratio of female to male friend counts across the categorical variable, age
Usually color or shape are the aesthetics to represent such changes over a categorical variable

What if we looked at color or shape over another numerical variable?
For example, friends are accumulated over time - so facebook tenure/days since registering on facebook is important 

Explore friend count, gender, age and tenure - using a 2D display like a scatterplot and bin a quantitative variables and compare the bins
In this case group users by the year that they joined

### Create a variable called year_joined in the pf data frame using the variable tenure and 2014 as the reference year. The variable year joined should contain the year that a user joined facebook.

```{r Third Quantitative Variable}
names(pf)
```
2014 is the current year,  so 2014 - no. of years on facebook 

if 2.2 years is the tenure, 3 should be subtracted from 2014 - ceiling() rounds up to the nearest integer 
floor() rounds down to the the nearest integer

```{r}
pf$year_joined = 2014 - ceiling(pf$tenure/365)
head(pf)

# OR
# pf$year_joined = floor(2014 - (pf$tenure/365))
```
### Testing the floor function- even when decimal is greater than 5 it will round down
```{r}
floor(3.8)
```

***

### Cut a Variable
Notes:

```{r Cut a Variable}
summary(pf$year_joined)
```
Value of the year joined are discrete and range is narrow, so let's table the values

```{r}
table(pf$year_joined)
```
This shows the distribution of users in each year joined
Notice that there is not much data about early joiners - To increase the data in each group of the year joined / tenure category 

'Cut' - useful to make discrete variables from continuous/ numerical ones, sometimes with quantile

### Cut the variable year_joined to create 4 buckets of users
# You need to create the following buckets for the
# new variable, year_joined.bucket

#        (2004, 2009]
#        (2009, 2011]
#        (2011, 2012]
#        (2012, 2014]

# Note that a parenthesis means exclude the year and a
# bracket means include the year.


```{r}

pf$year_joined.bucket <- cut(pf$year_joined,c(2004,2009,2011,2012,2014))
summary(pf$year_joined.bucket)
```
Should include useNA='ifany' , otherwise table() leaves out NA but summary displays NA

```{r}
table(pf$year_joined.bucket,useNA='ifany')
```


***

### Plotting it All Together
Notes:
This was the plot for median friend count vs age where each gender has its own line 
```{r Plotting it All Together}
ggplot(aes(x = age, y = friend_count),
       data = subset(pf, !is.na(gender))) +
  geom_line(aes(color = gender), stat = 'summary', fun.y = median)
```

### Create a similar plot of median friend count vs age where each year_joined.bucket has its own line in the graph
# Create a line graph of friend_count vs. age
# so that each year_joined.bucket is a line
# tracking the median user friend_count across
# age. This means you should have four different
# lines on your plot.

# You should subset the data to exclude the users
# whose year_joined.bucket is NA.

```{r}
ggplot(aes(x=age, y=friend_count), data=subset(pf,!is.na(pf$year_joined.bucket)))+geom_line(aes(color=year_joined.bucket),stat='summary', fun.y =median)
```
Comments:

Users with longer tenure tend to have higher friend counts with the exception of old users (80 and above) - red line at the top 
To put  these cohort specific medians in perspective we can change them to cohort specific means and plot the grand mean on here


### Plot the Grand Mean

# Write code to do the following:

# (1) Add another geom_line to code below
# to plot the grand mean of the friend count vs age.

# (2) Exclude any users whose year_joined.bucket is NA.

# (3) Use a different line type for the grand mean.

# As a reminder, the parameter linetype can take the values 0-6:

# 0 = blank, 1 = solid, 2 = dashed
# 3 = dotted, 4 = dotdash, 5 = longdash
# 6 = twodash
Notes:

```{r Plot the Grand Mean}
ggplot(aes(x=age, y=friend_count), data=subset(pf,!is.na(pf$year_joined.bucket)))+geom_line(aes(color=year_joined.bucket),stat='summary', fun.y =mean) + geom_line(linetype= 2, stat='summary',fun.y=mean)
```
Plotting the grand mean tells us that much of the data in the sample is from recent cohorts of joining - This is the type of more high level observation you want to  make as you explore data


***

### Friending Rate
Since the general pattern continues to hold after conditioning on each of the buckets of year_joined, we might increase our confidence that this observation isn't just an artifact of the time users have had to accumulate friends

Let's look at this relationship another way - we could look attenure and friend count as a rate instead
How many friends does a user have for each day since they started using the service - create a summary for this
Subset the data to only consider users with atleast one day of tenure

Once you have the summary, find the median and maximum friend rate

```{r Friending Rate}
with (subset(pf, tenure >= 1), summary (friend_count / tenure))

```
The max is 417 - definitely  an outlier because third quartile is only about 0.5
***

### Friendships Initiated
Users who have been on facebook longer have higher friend counts across ages
Wondering if friend requests are the same or different across groups
Do new users go on a friendship initiating spree? Or do users with more tenure initiate more friendships?

### Create a line plot of frienships initiated vs tenure - use age, tenure, friendships_initiated and year_joined.bucket
### The color of each part of your one line should correspond to each bucket of year joined. Subset to consider users with only one day of tenure


```{r Friendships Initiated}
ggplot(aes(x=tenure, y=friendships_initiated/tenure), data = subset(pf,tenure >=1)) + geom_line(aes(color= year_joined.bucket), stat='summary', fun.y = mean)
```
Users with more tenure typically seem to initiate less friendships

***
We are plotting y for every possible tenure value x - there is a lot of noise in the graph
We can adjust the noise by binning the x axis differently 

### Bias-Variance Tradeoff Revisited
Notes:

```{r Bias-Variance Tradeoff Revisited}


ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

ggplot(aes(x = 30 * round(tenure / 30), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

ggplot(aes(x = 90 * round(tenure / 90), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

```
binning values by the denominator in the round function and then transforming back to the natural scale with the constant in front. 
Same peaks from before, but this makes the graph much smoother in general 

Using 30 instead of 7,

```{r}
ggplot(aes(x = 30 * round(tenure / 30), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)
```
### Very high bias bit much lesser variance using 90,



```{r}
p1 = ggplot(aes(x=tenure, y=friendships_initiated/tenure), data = subset(pf,tenure >=1)) + geom_line(aes(color= year_joined.bucket), stat='summary', fun.y = mean)

p2 = ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

p3 = ggplot(aes(x = 30 * round(tenure / 30), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)

p4 = ggplot(aes(x = 90 * round(tenure / 90), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_line(aes(color = year_joined.bucket),
            stat = "summary",
            fun.y = mean)
library(gridExtra)
grid.arrange(p1,p2,p3,p4, ncol =1)
```
As bin size increases we see less noise on the plot
Our estimates for the mean are adjusted since we have more datapoints for our new values of tenure

We had earlier seen smoothers as a tool. Adda smoother to the above plot
Remove stat and fun.y parameters - geom_smooth automatically chooses the appropriate statistical methods based on the data


```{r}
ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_smooth(aes(color = year_joined.bucket))
```
We still see that friendships initiated declines as tenure increases


***

### Sean's NFL Fan Sentiment Study

Measuring fan sentiment for NFL teams 
Counted ratios of positive to negative words at five minute increments over 4 months of NFL season - some measurements are very high over 100, though avg is 2-3 

Computed moving average over 5 min increments - pooling over more measurements - makes them more reliable
Stilltoo frequent and noisy to tell a story
When aggregated upto a day of moving avg we start to see pattenrs emerge 
Knew what he was looking for
Wanted a model that predicts sentiment as a function of time - a natural spline

Colored lines - wins and losses over time and sentiment is tracked over time 
Gives an idea of why sentiment is upward or downward  
Optimistic in the beginning - agfter 3 losses sentiment dips

On the game day there is an abrupt change in sentiment - much sadder or happier at the end of a game - that is not displayed in the visual - expect to see discrete jumps - when that's not seen - you know it's a bad model
We need something more flexible

Use a 7 day moving avg - smooth it out over a week - tells a nice story  - big bumps on winning games, big dips when losses happen and periods of stability when you don't have info on how the team is doing 
- Nice depiction that took a ittle bit of averaging to get the story out 

Bias variance tradeoff??

When you are computing just a simple moving avg you're dealing with a very flexible statistic - not imposing an structure - just letting the data speak 

Plotted standard errors that were rolling  along with the data they were  gigantic - The mean sentiment for the season is somewhere in the three range

The std errors were over 2 or 3 - we can precisely say what happens at any point but variance on that estimate is huge 
As you start to add more lags, higher no of lags to the moving avg, we end up with smoother looking plots that have lower variance but are getting progressively more biased - because we are including more data that are actually not applicable to that exact point, in exchange we get a lower variance plot - one that doesn't move as wildly 

When you combine that with splines you end up fitting a model that has kind of best of both worlds - smoothing aspect of splines with the discrete jumps of what happens on game day

A spline where you add dummy variables for post game periods - smooth transition in between

- A resultant model that better account for the fact that game days are an important thing

***
In the facebook data analysis, we have come to understand the behavior of facebook users and how they use the facebook platform

### Introducing the Yogurt Data Set

```{r}
yo <- read.csv('yogurt.csv')
str(yo)
```

```{r}
# Change id from int  to factor
yo$id <- factor(yo$id)
str(yo)
```

***

### Histograms Revisited
Notes:
```{r}
qplot(x=price, data=yo,  fill =I('#F79420'))
```


```{r Histograms Revisited}
qplot(x=price, data=yo, binwidth=1, fill =I('#F79420'))
```
There is n important discreteness to this distribution. There appear to be prices at which there are many observations, no observations in adjacent prices 
This makes sense that prices are set in a way that applies to many of the consumers  - some purchases involve much lower prices 
If we are interested in price sensitivity we definitely want to consider the sort of variation 

###Choosing binwidth of 10, you'll miss the discreteness and empty spaces - for this data binwidth =10 is a very biased model

```{r}
qplot(x=price, data=yo, binwidth=10, fill =I('#F79420'))
```

***

### Number of Purchases
Notes:

```{r Number of Purchases}
summary(yo)
```
3rd quartile of price = max price - clue to discreetness 

Can also see discreetness by how many distinct prices there are in the dataset

```{r}
length(unique(yo$price))
```
There are 20 different prices 
```{r}
table(yo$price)
```

###On a given purchase occasion, how many yogurts does a household purchase?

###Combine sales for different flavors into one variable for each household

### Create a variable all.purchases that gives the total counts of yogurt for each observation or household purchase 

```{r}
yo$all.purchases= NA
yo=transform(yo, all.purchases= strawberry + blueberry + pina.colada + plain + mixed.berry)
head(yo)
```

***

### Prices over Time
Notes:

```{r Prices over Time}
qplot(x=all.purchases, data = yo, binwidth =1, fill =I('#099DD9'))
```
Most households buy one or two yogurts at a time 

We have data on the same households over time


# Create a scatterplot of price vs time.

# This will be an example of a time series plot.

# Resolve overplotting issues by using
# techniques you learned in Lesson 4.

# What are some things that you notice?

Adding jitter to reduce overplotting
```{r}
ggplot(aes(x=time, y=price), data=yo)+geom_jitter(alpha=1/20, shape = 21, fill = I('#F79420'))
```
The modal or most common prices seem to increase over time
We also see some lower price points scattered about the graph - These maybe due to sale, or perhaps buyers using coupons that bring down the price of yogurt

***

### Sampling Observations
How can we proceed differently with this type of  data set??

A data set with multiple observations of the same units, often useful to work with a sample of the units so that it's easy to display the raw data for that sample

Look at a small sample of households in more detail - we know what kind of within and between household variation we  are working with 
The sub sample analysis might come before trying to use within household variation as part of a model
For eg, to model consumr preferences at variety 
How often household buy yogurt 
How often they buy multiple items 
What prices they are buying yogurt at 

Let's  pick 16 households at random and take a closer look 

### Code up the above exploration 

-Set the seed and sample 16 households
-Sample from the levels because levels has the unique sample ids in the dataset

```{r}
# Make the exploration reproducible
set.seed(4230)
sample.ids <- sample(levels(yo$id), 16) 
```
Plot each purchase occasion for each of the households that we sampled

-Subset the dataframe to include only the sampled ids
-Facet wrap id - we want 
- A line plot
- size parameter adds more detail to the dataset - no of yogurts purchased is the size of the point 

%in% loops over all the ids in sample.ids
Note: x %in% y returns a logical (boolean) vector the same length as x that says whether each entry in x appears in y. That is, for each entry in x, it checks to see whether it is in y. This allows us to subset the data so we get all the purchases occasions for the households in the sample. Then, we create scatterplots of price vs. time and facet by the sample id. 

Use the pch or shape parameter to specify the symbol when plotting points. Scroll down to 'Plotting Points' on QuickR's Graphical Parameters.
```{r}
ggplot (aes(x=time, y=price), data = subset(yo, id %in% sample.ids))+
  facet_wrap(~id)+
  geom_line()+
  geom_point(aes(size = all.purchases), pch =1)
```

This creates a panel plot for each id of the household - we get panels for each of the households
Horizontal line - not so much variation in how often each household buys yogurt 
Larger circles - more quantity
For most households the money spent on yogurt (price) tends to be steady or increases over time except few like 
2nd row last plot - there is a dcrease in price
2nd row 3rd plot - the household may be using coupons to drive the price down

Coupon info can be paired with this data for more info

Using another seed number, pick 16 households again and make the same plot

```{r}
set.seed(200)
sample2.ids <- sample(levels(yo$id), 16) 
ggplot (aes(x=time, y=price), data = subset(yo, id %in% sample2.ids))+
  facet_wrap(~id)+
  geom_line()+
  geom_point(aes(size = all.purchases), pch =1)
```


***

### The Limits of Cross Sectional Data
General idea is that if we have observations over time we can facet by the primary unit, case or individual in the dataset
For the yogurt data it was the households we were faceting over
This faceted time series plot is something we can't generate with pseudo facebook dataset since we don't have data on our sample of users over time - get back to that plot and see the limitation

```{r}
ggplot(aes(x = 7 * round(tenure / 7), y = friendships_initiated / tenure),
       data = subset(pf, tenure > 0)) +
  geom_smooth(aes(color = year_joined.bucket))

```
This data isn't great for examining the process of friending over time
The dataset is just a crosssection - one snapshot at a fixed pt that tells the characteristics of individuals, not the individuals over, say a year
But if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure
This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases
***

### Many Variables
Another technique that you can use in your work as data analyst

Much of the analysis so far focused on some pre-chosen variable, relationship or question of interest
We then used EDA to let those chosen variables speak and surprise us
When analyzing the relationship between 2 variables we look to incorporate more vars into the analysis to improve it
For eg, by seeing whether a particular relationship is consistent across values of those other variables
In choosing a third or fourth variable to plot we relied on our domain knowledge 
But often we might want visualizations or summaries to help us identify such auxilary variables 
In some analyses we may plan to make use of a large number of variables 
Perhaps we are planning on predicting one variable with 10,20 or 100s of others or maybe we want to summarize a large set of variables into a smaller set of dimensions, or perhaps we're looking for interesting relationships among a large set of variables 
In such cases, we can help speed up the EDA by producing many plots or comparisons at once 
This could be one way to let the dataset as a whole speak in part by drawing our attention to variables we didn't have a preexisting interest in

***

### Scatterplot Matrix
Let the data speak to determine variables of interest
A tool - create a no of scatterplots automatically - scatterplot matrix

grid of scatterplot between every pair of vars  - they are great but not suited to all var types, eg categorical variables
So there are other types of visualizations instead of scatter, like boxplot / histogram for categorical vars

Need the GGally package

```{r}
#install.packages('GGally')
```
SCATTERPLOT MATRIX
```{r}
library(GGally)
theme_set(theme_minimal(20))

set.seed(1836)
# We are going to sample from our dataset
# We don't need  all vars - id, year_joined (derived from tenure), year_joined.bucket (categorical) so subset the dataframe and sample from that

pf_subset <- pf[ , c(2:15)]
names(pf_subset)
```
 
Use the GGpairs function inside of GGally to create the scatterplot matrix - this could even take an hour to run 

```{r}
ggpairs(pf_subset[sample.int(nrow(pf_subset),1000),])
```

If the plot takes a long time to render or if you want to see some of the scatterplot matrix, then only examine a smaller number of variables. You can use the following code or select fewer variables. We recommend including gender (the 6th variable)!

pf_subset = pf[, c('age', 'dob_year', 'dob_month', 'gender', 'tenure')]

You can also select a subset using the subset() function and the "select" argument:

pf_subset <- subset(pf, select = -c(userid, year_joined, year_joined_bucket))

The - sign in the "select" value indicates all but the listed columns.

You may find in your matrix that variable labels are on the outer edges of the scatterplot matrix, rather than on the diagonal. If you want labels in the diagonal, you can set the axisLabels = 'internal' argument in your ggpairs command.


Check pdf for an image of the matrix

For age and DOB year, the correlation is actually negative -1
One number summaries of the different relationships can be seen - correlation coeff - in the matrix 

ggpairs uses different plot types for different combinations of variables - hence we have histograms and scatterplots

For counts of likes we might want a log scale - but here the plot isn't as nice as it would be if fine tuned

But this scatterplot matrix is just a starting point for many analyses
***

### Even More Variables

The scatterplot matrix will be extremely helpful when there are too many variables
Eg, genomic data - 1000s of genetic measurements for each sample 
ID genes associated with a disease

***

### Heat Maps
Notes:

```{r}
nci <- read.table("nci.tsv")

#changing column names to 1-64 to produce a nicer plot
colnames(nci) <- c(1:64)
head(nci)
```

Display each combination of gene and sample case, the difference in gene expression and sample from the baseline
Display combinations where a gene is over expressed in red, underexpressed in blue


Melt the data into long format
```{r}
library(reshape2)
nci.long.samp <- melt(as.matrix(nci[1:200,]))
names(nci.long.samp) <- c("gene", "case", "value")
head(nci.long.samp)


```

```{r}
ggplot(aes(y = gene, x = case, fill = value),
  data = nci.long.samp) +
  geom_tile() +
  scale_fill_gradientn(colours = colorRampPalette(c("blue", "red"))(100))
```
Even in the dense display we are only showing 200 genes of 6000
Genomic data is getting larger and complex 

Fo eg, internet companies run randomizing experiments - A?B testing - difference in treatment outcome can be computed for many metrics of interest - many expts and many metrics - data looks similar to genomic data


***

### Analyzing Three of More Variables
Reflection:

***

Click **KnitHTML** to see all of your hard work and to have an html
page of this lesson, your answers, and your notes!

